{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753e0ecb",
   "metadata": {},
   "source": [
    "# Can we trick the experts?\n",
    "Full, reproducible pipeline for Likert + 2AFC expert validation **with seaborn plots**, saved outputs, and a one-click PDF export."
   ]
  },
  {
   "cell_type": "code",
   "id": "481ee9f1",
   "metadata": {},
   "source": [
    "# == Setup ==\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Configure paths\n",
    "LIKERT_CSV = '../results/expert_validation/likert_ratings.csv'\n",
    "AFC_CSV = '../results/expert_validation/2afc_choices.csv'\n",
    "RESULTS_DIR = '../results/expert_validation'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_theme(style='whitegrid', context='talk')\n",
    "PALETTE = sns.color_palette('Set2')\n",
    "DIVERGING = sns.diverging_palette(260, 10, as_cmap=True)\n",
    "\n",
    "# Show plots inline & make them crisp\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "\n",
    "print(\"Paths set.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5885d974",
   "metadata": {},
   "source": [
    "\n",
    "# == Helpers ==\n",
    "def save_fig(fig, path):\n",
    "    # Save and also show the figure\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=220, bbox_inches=\"tight\")\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def clopper_pearson_ci(k, n, alpha=0.05):\n",
    "    if n == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    lower = stats.beta.ppf(alpha/2, k, n-k+1)\n",
    "    upper = stats.beta.ppf(1 - alpha/2, k+1, n-k)\n",
    "    return (lower, upper)\n",
    "\n",
    "def cliffs_delta(x, y):\n",
    "    x = np.array(x); y = np.array(y)\n",
    "    n1, n2 = len(x), len(y)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return np.nan\n",
    "    gt = sum((xi > yi) for xi in x for yi in y)\n",
    "    lt = sum((xi < yi) for xi in x for yi in y)\n",
    "    return (gt - lt) / (n1 * n2)\n",
    "\n",
    "def roc_auc_from_scores(scores, labels):\n",
    "    s = pd.Series(scores); l = pd.Series(labels)\n",
    "    df = pd.DataFrame({\"s\": s, \"l\": l}).dropna()\n",
    "    if df[\"l\"].nunique() != 2:\n",
    "        return np.nan\n",
    "    real = df[df.l == 1][\"s\"]; synth = df[df.l == 0][\"s\"]\n",
    "    if len(real) == 0 or len(synth) == 0:\n",
    "        return np.nan\n",
    "    U, _ = stats.mannwhitneyu(real, synth, alternative=\"two-sided\")\n",
    "    return U / (len(real) * len(synth))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4fae609",
   "metadata": {},
   "source": [
    "## 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "id": "540f62ba",
   "metadata": {},
   "source": [
    "\n",
    "likert = pd.read_csv(LIKERT_CSV)\n",
    "afc = pd.read_csv(AFC_CSV)\n",
    "\n",
    "# Standardize columns\n",
    "likert = likert.rename(columns={\n",
    "    \"user\": \"user_name\",\n",
    "    \"expert\": \"user_name\",\n",
    "    \"rating\": \"rating_score\",\n",
    "    \"image\": \"image_filename\",\n",
    "    \"type\": \"image_type\"\n",
    "})\n",
    "afc = afc.rename(columns={\n",
    "    \"user\": \"user_name\",\n",
    "    \"expert\": \"user_name\",\n",
    "})\n",
    "\n",
    "if \"rating_score\" in likert.columns:\n",
    "    likert[\"rating_score\"] = pd.to_numeric(likert[\"rating_score\"], errors=\"coerce\")\n",
    "\n",
    "display(likert.head())\n",
    "display(afc.head())\n",
    "print(f\"{len(likert)} Likert ratings, {len(afc)} 2AFC trials\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d185eec",
   "metadata": {},
   "source": [
    "## 2) Likert analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "3476c4f4",
   "metadata": {},
   "source": [
    "\n",
    "# Per-expert histograms\n",
    "bins = np.arange(0.5, 5.6, 1.0)\n",
    "if \"user_name\" in likert and len(likert[\"user_name\"].unique()) > 0:\n",
    "    g = sns.displot(\n",
    "        data=likert.dropna(subset=[\"rating_score\"]),\n",
    "        x=\"rating_score\",\n",
    "        col=\"user_name\",\n",
    "        col_wrap=3,\n",
    "        bins=bins,\n",
    "        discrete=True,\n",
    "        facet_kws=dict(sharex=True, sharey=False),\n",
    "        color=PALETTE[0]\n",
    "    )\n",
    "    g.set_axis_labels(\"Likert rating\", \"Count\")\n",
    "    g.set_titles(\"Likert distribution — {col_name}\")\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(\"Per-expert Likert distributions\")\n",
    "    g.savefig(os.path.join(RESULTS_DIR, \"likert_hist_per_expert.png\"), dpi=220, bbox_inches=\"tight\")\n",
    "    display(g.fig)\n",
    "\n",
    "# Overall by image_type\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.histplot(\n",
    "    data=likert.dropna(subset=[\"rating_score\"]),\n",
    "    x=\"rating_score\", hue=\"image_type\",\n",
    "    multiple=\"dodge\", bins=bins, shrink=0.9,\n",
    "    palette=PALETTE, edgecolor=\"black\", ax=ax\n",
    ")\n",
    "sns.kdeplot(\n",
    "    data=likert.dropna(subset=[\"rating_score\"]),\n",
    "    x=\"rating_score\", hue=\"image_type\",\n",
    "    common_norm=False, fill=False, ax=ax, palette=PALETTE, lw=2\n",
    ")\n",
    "ax.set_title(\"Likert distribution by image_type (overall)\")\n",
    "ax.set_xlabel(\"Likert rating\"); ax.set_ylabel(\"Count\")\n",
    "save_fig(fig, os.path.join(RESULTS_DIR, \"likert_hist_by_type_overall.png\"))\n",
    "\n",
    "# Per-expert split by type\n",
    "for expert, sub in likert.groupby(\"user_name\"):\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.violinplot(\n",
    "        data=sub, x=\"image_type\", y=\"rating_score\",\n",
    "        inner=None, palette=PALETTE, ax=ax, cut=0\n",
    "    )\n",
    "    sns.boxplot(\n",
    "        data=sub, x=\"image_type\", y=\"rating_score\",\n",
    "        width=0.25, showcaps=True, fliersize=2, boxprops={'zorder': 3},\n",
    "        ax=ax, color=\"white\"\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=sub, x=\"image_type\", y=\"rating_score\",\n",
    "        color=\"k\", size=3, alpha=0.3, dodge=True, ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"Likert by type — {expert}\")\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"Rating\")\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, f\"likert_by_type_{expert}.png\"))\n",
    "\n",
    "# Stats: real vs synthetic\n",
    "real_scores = likert.loc[likert.image_type==\"real\", \"rating_score\"].dropna()\n",
    "synth_scores = likert.loc[likert.image_type==\"synthetic\", \"rating_score\"].dropna()\n",
    "mw_stat, mw_p = (np.nan, np.nan); delta = np.nan; auc = np.nan\n",
    "if len(real_scores) > 0 and len(synth_scores) > 0:\n",
    "    mw_stat, mw_p = stats.mannwhitneyu(real_scores, synth_scores, alternative=\"two-sided\")\n",
    "    delta = cliffs_delta(real_scores, synth_scores)\n",
    "    auc = roc_auc_from_scores(pd.concat([real_scores, synth_scores]),\n",
    "                              [1]*len(real_scores) + [0]*len(synth_scores))\n",
    "display(pd.DataFrame({\n",
    "    \"real_mean\":[real_scores.mean() if len(real_scores) else np.nan],\n",
    "    \"synth_mean\":[synth_scores.mean() if len(synth_scores) else np.nan],\n",
    "    \"MW_U\":[mw_stat], \"MW_p\":[mw_p], \"Cliffs_delta\":[delta], \"AUC\":[auc]\n",
    "}))\n",
    "\n",
    "# Inter-expert agreement\n",
    "experts = sorted(likert[\"user_name\"].dropna().unique().tolist())\n",
    "pivot = likert.pivot_table(index=\"image_filename\", columns=\"user_name\", values=\"rating_score\", aggfunc=\"mean\")\n",
    "\n",
    "for corr_method, fname, title in [\n",
    "    (\"spearman\", \"likert_agreement_spearman.png\", \"Inter-expert agreement (Spearman)\"),\n",
    "    (\"kendall\", \"likert_agreement_kendall.png\", \"Inter-expert agreement (Kendall)\")\n",
    "]:\n",
    "    corr = pivot.corr(method=corr_method)\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 5.5))\n",
    "    sns.heatmap(\n",
    "        corr, mask=mask, vmin=-1, vmax=1, cmap=DIVERGING, center=0,\n",
    "        square=True, cbar_kws={\"shrink\": .75, \"label\": \"Correlation\"},\n",
    "        linewidths=.5, annot=True, fmt=\".2f\", ax=ax\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    plt.xticks(rotation=45, ha=\"right\"); plt.yticks(rotation=0)\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, fname))\n",
    "\n",
    "# Likert AUC per expert\n",
    "expert_likert_auc = []\n",
    "for expert, sub in likert.groupby(\"user_name\"):\n",
    "    a = roc_auc_from_scores(sub[\"rating_score\"], (sub[\"image_type\"]==\"real\").astype(int).values)\n",
    "    expert_likert_auc.append({\"user_name\": expert, \"likert_auc_real_vs_synth\": a})\n",
    "expert_likert_auc = pd.DataFrame(expert_likert_auc)\n",
    "\n",
    "if len(expert_likert_auc):\n",
    "    fig, ax = plt.subplots(figsize=(7, 3.8))\n",
    "    order = expert_likert_auc.sort_values(\"likert_auc_real_vs_synth\", ascending=False)\n",
    "    sns.barplot(\n",
    "        data=order,\n",
    "        y=\"user_name\", x=\"likert_auc_real_vs_synth\",\n",
    "        hue=\"user_name\", dodge=False, legend=False,\n",
    "        palette=PALETTE, ax=ax\n",
    "    )\n",
    "    ax.set_xlabel(\"Likert AUC (real > synthetic)\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    for i, v in enumerate(order[\"likert_auc_real_vs_synth\"]):\n",
    "        ax.text(v + 0.01, i, f\"{v:.2f}\", va=\"center\")\n",
    "    ax.set_title(\"Who separates real vs synthetic (Likert AUC)?\")\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, \"likert_auc_per_expert.png\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fc4ed7b",
   "metadata": {},
   "source": [
    "## 3) 2AFC analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b269758",
   "metadata": {},
   "source": [
    "\n",
    "afc[\"pair_kind\"] = np.where(\n",
    "    afc[\"image1_type\"] == afc[\"image2_type\"],\n",
    "    afc[\"image1_type\"],\n",
    "    \"mixed\"\n",
    ")\n",
    "afc[\"is_correct\"] = np.where(\n",
    "    afc[\"pair_kind\"]==\"mixed\",\n",
    "    (afc[\"chosen_image_type\"]==\"real\").astype(int),\n",
    "    np.nan\n",
    ")\n",
    "afc[\"chose_left\"] = np.where(\n",
    "    afc[\"pair_kind\"]!=\"mixed\",\n",
    "    (afc[\"chosen_image_filename\"]==afc[\"image1_filename\"]).astype(int),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "mixed = afc[afc[\"pair_kind\"]==\"mixed\"].copy()\n",
    "overall_n = mixed[\"is_correct\"].notna().sum()\n",
    "overall_k = mixed[\"is_correct\"].sum()\n",
    "overall_acc = (overall_k / overall_n) if overall_n else np.nan\n",
    "overall_ci = clopper_pearson_ci(int(overall_k), int(overall_n)) if overall_n else (np.nan, np.nan)\n",
    "overall_pval = stats.binomtest(int(overall_k), int(overall_n), p=0.5, alternative=\"two-sided\").pvalue if overall_n else np.nan\n",
    "\n",
    "per_expert_rows = []\n",
    "for expert, sub in mixed.groupby(\"user_name\"):\n",
    "    n = sub[\"is_correct\"].notna().sum()\n",
    "    k = sub[\"is_correct\"].sum()\n",
    "    acc = k/n if n else np.nan\n",
    "    lo, hi = clopper_pearson_ci(int(k), int(n)) if n else (np.nan, np.nan)\n",
    "    pval = stats.binomtest(int(k), int(n), p=0.5, alternative=\"two-sided\").pvalue if n else np.nan\n",
    "    per_expert_rows.append({\n",
    "        \"user_name\": expert, \"n_trials\": n, \"k_correct\": int(k),\n",
    "        \"accuracy\": acc, \"ci_low\": lo, \"ci_high\": hi, \"binom_p_vs_0.5\": pval\n",
    "    })\n",
    "per_expert_acc = pd.DataFrame(per_expert_rows).sort_values(\"accuracy\", ascending=False)\n",
    "display(per_expert_acc)\n",
    "\n",
    "# Bar + CI\n",
    "if len(per_expert_acc):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    sns.barplot(\n",
    "        data=per_expert_acc,\n",
    "        y=\"user_name\", x=\"accuracy\",\n",
    "        hue=\"user_name\", dodge=False, legend=False,\n",
    "        palette=PALETTE, ax=ax\n",
    "    )\n",
    "    for i, r in per_expert_acc.iterrows():\n",
    "        ax.errorbar(\n",
    "            x=r[\"accuracy\"], y=i,\n",
    "            xerr=[[r[\"accuracy\"] - r[\"ci_low\"]], [r[\"ci_high\"] - r[\"accuracy\"]]],\n",
    "            fmt=\"none\", ecolor=\"k\", elinewidth=1.2, capsize=4\n",
    "        )\n",
    "        ax.text(min(r[\"ci_high\"]+0.02, 1.02), i, f\"{r['accuracy']:.2f}\", va=\"center\")\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.set_xlabel(\"2AFC accuracy (95% CI)\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_title(\"Which experts are fooled? (higher = better at spotting real)\")\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, \"afc_accuracy_per_expert.png\"))\n",
    "\n",
    "mixed[\"synthetic_was_chosen\"] = (mixed[\"chosen_image_type\"]==\"synthetic\").astype(int)\n",
    "fool_rate_overall = mixed[\"synthetic_was_chosen\"].mean() if len(mixed) else np.nan\n",
    "\n",
    "fool_rows = []\n",
    "for expert, sub in mixed.groupby(\"user_name\"):\n",
    "    fr = sub[\"synthetic_was_chosen\"].mean()\n",
    "    fool_rows.append({\"user_name\": expert, \"fool_rate_synth_chosen\": fr, \"n_trials\": len(sub)})\n",
    "per_expert_fool = pd.DataFrame(fool_rows).sort_values(\"fool_rate_synth_chosen\", ascending=False)\n",
    "display(per_expert_fool)\n",
    "\n",
    "# Fool rate bar\n",
    "if len(per_expert_fool):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.2))\n",
    "    sns.barplot(\n",
    "        data=per_expert_fool,\n",
    "        y=\"user_name\", x=\"fool_rate_synth_chosen\",\n",
    "        hue=\"user_name\", dodge=False, legend=False,\n",
    "        palette=PALETTE, ax=ax\n",
    "    )\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"Fool rate (synthetic chosen in mixed pairs)\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    for i, v in enumerate(per_expert_fool[\"fool_rate_synth_chosen\"]):\n",
    "        ax.text(v + 0.01, i, f\"{v:.2f}\", va=\"center\")\n",
    "    ax.set_title(\"Which experts are tricked by synthetic images?\")\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, \"afc_fool_rate_per_expert.png\"))\n",
    "\n",
    "# Controls: position bias\n",
    "controls = afc[afc[\"pair_kind\"]!=\"mixed\"].copy()\n",
    "pos_rows = []\n",
    "for expert, sub in controls.groupby(\"user_name\"):\n",
    "    n = sub[\"chose_left\"].notna().sum()\n",
    "    left_rate = sub[\"chose_left\"].mean() if n else np.nan\n",
    "    pval = stats.binomtest(int(sub[\"chose_left\"].sum()), int(n), p=0.5).pvalue if n else np.nan\n",
    "    pos_rows.append({\"user_name\": expert, \"left_bias_rate_controls\": left_rate, \"n_control_trials\": n, \"p_vs_0.5\": pval})\n",
    "per_expert_posbias = pd.DataFrame(pos_rows).sort_values(\"left_bias_rate_controls\", ascending=False)\n",
    "display(per_expert_posbias)\n",
    "\n",
    "if len(per_expert_posbias):\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 3.6))\n",
    "    sns.barplot(\n",
    "        data=per_expert_posbias,\n",
    "        y=\"user_name\", x=\"left_bias_rate_controls\",\n",
    "        hue=\"user_name\", dodge=False, legend=False,\n",
    "        palette=PALETTE, ax=ax\n",
    "    )\n",
    "    ax.axvline(0.5, ls=\"--\", color=\"k\", lw=1)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"Chose-left rate (controls)\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    for i, r in per_expert_posbias.iterrows():\n",
    "        star = \" *\" if pd.notna(r[\"p_vs_0.5\"]) and r[\"p_vs_0.5\"] < 0.05 else \"\"\n",
    "        ax.text(r[\"left_bias_rate_controls\"] + 0.01, i,\n",
    "                f\"{r['left_bias_rate_controls']:.2f}{star}\", va=\"center\")\n",
    "    ax.set_title(\"Control trials: position bias (left/right)\")\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, \"control_position_bias.png\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a077855a",
   "metadata": {},
   "source": [
    "## 4) Linking Likert ↔ 2AFC"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba1fa03b",
   "metadata": {},
   "source": [
    "\n",
    "cross = per_expert_fool.merge(expert_likert_auc, on=\"user_name\", how=\"left\")\n",
    "rho, rho_p = stats.spearmanr(\n",
    "    cross[\"fool_rate_synth_chosen\"], cross[\"likert_auc_real_vs_synth\"],\n",
    "    nan_policy=\"omit\"\n",
    ") if len(cross) > 1 else (np.nan, np.nan)\n",
    "\n",
    "if len(cross):\n",
    "    jp = sns.jointplot(\n",
    "        data=cross,\n",
    "        x=\"likert_auc_real_vs_synth\",\n",
    "        y=\"fool_rate_synth_chosen\",\n",
    "        kind=\"reg\", scatter_kws=dict(s=40, alpha=0.8),\n",
    "        height=5, color=PALETTE[1]\n",
    "    )\n",
    "    jp.ax_joint.set_xlim(0, 1); jp.ax_joint.set_ylim(0, 1)\n",
    "    jp.ax_joint.set_xlabel(\"Likert AUC (real vs synthetic separation)\")\n",
    "    jp.ax_joint.set_ylabel(\"Fool rate (synthetic chosen)\")\n",
    "    jp.ax_joint.set_title(f\"Do Likert separations predict who gets fooled? ρ={rho:.2f}, p={rho_p:.3f}\")\n",
    "    jp.figure.subplots_adjust(top=0.88)\n",
    "    jp.figure.savefig(os.path.join(RESULTS_DIR, \"likert_auc_vs_foolrate.png\"), dpi=220, bbox_inches=\"tight\")\n",
    "    display(jp.figure)\n",
    "else:\n",
    "    print(\"Not enough data for cross-link plot.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52c59140",
   "metadata": {},
   "source": [
    "## 5) Per-synthetic image foolability & Likert realism"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb424b31",
   "metadata": {},
   "source": [
    "\n",
    "mixed_pairs = mixed.copy()\n",
    "def synthetic_image_filename(row):\n",
    "    if row[\"image1_type\"]==\"synthetic\":\n",
    "        return row[\"image1_filename\"]\n",
    "    else:\n",
    "        return row[\"image2_filename\"]\n",
    "\n",
    "mixed_pairs[\"synthetic_image\"] = mixed_pairs.apply(synthetic_image_filename, axis=1)\n",
    "mixed_pairs[\"synthetic_won\"] = (mixed_pairs[\"chosen_image_type\"]==\"synthetic\").astype(int)\n",
    "\n",
    "per_synth_image = mixed_pairs.groupby(\"synthetic_image\")[\"synthetic_won\"].agg([\"mean\",\"count\"]).reset_index()\n",
    "per_synth_image = per_synth_image.rename(columns={\"mean\":\"foolability\", \"count\":\"n_trials\"}).sort_values(\"foolability\", ascending=False)\n",
    "\n",
    "likert_by_image = likert.groupby([\"image_filename\",\"image_type\"])[\"rating_score\"].agg([\"mean\",\"std\",\"count\"]).reset_index()\n",
    "likert_by_image = likert_by_image.rename(columns={\"mean\":\"mean_rating\",\"std\":\"std_rating\",\"count\":\"n_ratings\"})\n",
    "\n",
    "synth_likert = likert_by_image[likert_by_image[\"image_type\"]==\"synthetic\"].copy()\n",
    "per_synth_image = per_synth_image.merge(\n",
    "    synth_likert[[\"image_filename\",\"mean_rating\",\"n_ratings\"]],\n",
    "    left_on=\"synthetic_image\", right_on=\"image_filename\", how=\"left\"\n",
    ").drop(columns=[\"image_filename\"])\n",
    "\n",
    "display(per_synth_image.head(15))\n",
    "\n",
    "# Plot top 15\n",
    "if len(per_synth_image):\n",
    "    topN = per_synth_image.head(15).copy()\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    sns.barplot(\n",
    "        data=topN,\n",
    "        y=\"synthetic_image\", x=\"foolability\",\n",
    "        color=PALETTE[2], ax=ax\n",
    "    )\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"Foolability (win rate vs real in mixed pairs)\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    for i, r in enumerate(topN.itertuples()):\n",
    "        ax.text(r.foolability + 0.01, i, f\"{r.foolability:.2f} (n={r.n_trials})\", va=\"center\")\n",
    "    ax.set_title(\"Most convincing synthetic images (Top 15)\")\n",
    "    save_fig(fig, os.path.join(RESULTS_DIR, \"per_synthetic_image_foolability_top15.png\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d21dc490",
   "metadata": {},
   "source": [
    "## 6) Save summary tables"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3eec1c7",
   "metadata": {},
   "source": [
    "\n",
    "overall_row = {\n",
    "    \"overall_likert_real_mean\": float(real_scores.mean()) if len(real_scores)>0 else np.nan,\n",
    "    \"overall_likert_synth_mean\": float(synth_scores.mean()) if len(synth_scores)>0 else np.nan,\n",
    "    \"likert_MannWhitneyU\": float(mw_stat) if not np.isnan(mw_stat) else np.nan,\n",
    "    \"likert_MW_pvalue\": float(mw_p) if not np.isnan(mw_p) else np.nan,\n",
    "    \"likert_cliffs_delta\": float(delta) if not np.isnan(delta) else np.nan,\n",
    "    \"likert_auc_real_vs_synth\": float(auc) if not np.isnan(auc) else np.nan,\n",
    "    \"afc_overall_accuracy\": float(overall_acc) if not np.isnan(overall_acc) else np.nan,\n",
    "    \"afc_overall_ci_low\": float(overall_ci[0]) if not np.isnan(overall_ci[0]) else np.nan,\n",
    "    \"afc_overall_ci_high\": float(overall_ci[1]) if not np.isnan(overall_ci[1]) else np.nan,\n",
    "    \"afc_overall_p_vs_0.5\": float(overall_pval) if not np.isnan(overall_pval) else np.nan,\n",
    "    \"afc_overall_fool_rate\": float(fool_rate_overall) if not np.isnan(fool_rate_overall) else np.nan,\n",
    "    \"n_experts\": int(len(experts)),\n",
    "    \"n_likert\": int(len(likert)),\n",
    "    \"n_afc\": int(len(afc)),\n",
    "    \"n_afc_mixed\": int(len(mixed)),\n",
    "    \"n_afc_controls\": int(len(controls)),\n",
    "}\n",
    "summary_overall = pd.DataFrame([overall_row])\n",
    "\n",
    "per_expert_all = per_expert_acc.merge(per_expert_fool[[\"user_name\",\"fool_rate_synth_chosen\"]], on=\"user_name\", how=\"left\")\n",
    "per_expert_all = per_expert_all.merge(expert_likert_auc, on=\"user_name\", how=\"left\")\n",
    "per_expert_all = per_expert_all.merge(per_expert_posbias[[\"user_name\",\"left_bias_rate_controls\",\"n_control_trials\",\"p_vs_0.5\"]], on=\"user_name\", how=\"left\")\n",
    "\n",
    "summary_overall.to_csv(os.path.join(RESULTS_DIR, \"summary_overall.csv\"), index=False)\n",
    "per_expert_all.to_csv(os.path.join(RESULTS_DIR, \"summary_per_expert.csv\"), index=False)\n",
    "per_synth_image.to_csv(os.path.join(RESULTS_DIR, \"summary_per_synthetic_image.csv\"), index=False)\n",
    "likert_by_image.to_csv(os.path.join(RESULTS_DIR, \"likert_by_image.csv\"), index=False)\n",
    "\n",
    "display(summary_overall)\n",
    "display(per_expert_all.head())\n",
    "display(likert_by_image.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8fef25a1",
   "metadata": {},
   "source": [
    "## 7) Auto-generate narrative report (.md)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a10872fb",
   "metadata": {},
   "source": [
    "\n",
    "def pct(x): return f\"{100*x:.1f}%\" if pd.notna(x) else \"n/a\"\n",
    "def fmt(x, nd=3): return f\"{x:.{nd}f}\" if pd.notna(x) else \"n/a\"\n",
    "\n",
    "top_tricked = per_expert_all.sort_values(\"fool_rate_synth_chosen\", ascending=False).head(5)\n",
    "top_discriminators = per_expert_all.sort_values(\"likert_auc_real_vs_synth\", ascending=False).head(5)\n",
    "\n",
    "lines = []\n",
    "lines.append(f\"# Can we trick the experts?\\n\")\n",
    "lines.append(f\"_Auto-report generated {datetime.now().isoformat(timespec='seconds')}_\\n\")\n",
    "lines.append(\"## TL;DR\\n\")\n",
    "lines.append(f\"- **2AFC overall accuracy** on mixed pairs: **{pct(overall_acc)}** (95% CI {pct(overall_ci[0])}–{pct(overall_ci[1])}; p vs 50/50 chance = {fmt(overall_pval)}).\")\n",
    "lines.append(f\"- **Fool rate** (synthetic chosen in mixed pairs): **{pct(fool_rate_overall)}**.\")\n",
    "lines.append(f\"- **Likert separation** (AUC real > synthetic): **{fmt(auc)}** (Cliff's delta {fmt(delta)}; Mann–Whitney p={fmt(mw_p)}).\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Which experts get tricked?\\n\")\n",
    "for _, r in top_tricked.iterrows():\n",
    "    lines.append(f\"- **{r['user_name']}**: fool rate {pct(r['fool_rate_synth_chosen'])}, 2AFC accuracy {pct(r['accuracy'])} (n={int(r['n_trials'])}).\")\n",
    "lines.append(\"\\n## Who can tell apart real vs fake from Likert alone?\\n\")\n",
    "for _, r in top_discriminators.iterrows():\n",
    "    lines.append(f\"- **{r['user_name']}**: Likert AUC {fmt(r['likert_auc_real_vs_synth'])}; 2AFC acc {pct(r['accuracy'])}.\")\n",
    "lines.append(\"\\n## What do the control tests reveal?\\n\")\n",
    "lines.append(\"- Control trials are pairs where **both images are the same type** (both real or both synthetic).\")\n",
    "if len(controls):\n",
    "    mean_left = per_expert_posbias[\"left_bias_rate_controls\"].mean()\n",
    "    lines.append(f\"- Average **left/right position bias** in controls: chose-left rate {pct(mean_left)} across experts.\")\n",
    "    biased = per_expert_posbias[per_expert_posbias[\"p_vs_0.5\"] < 0.05]\n",
    "    if len(biased):\n",
    "        names = \", \".join(biased[\"user_name\"].tolist())\n",
    "        lines.append(f\"- Experts showing **significant position bias** (p<0.05): {names}.\")\n",
    "    else:\n",
    "        lines.append(\"- No expert shows a statistically significant position bias (p<0.05).\")\n",
    "else:\n",
    "    lines.append(\"- No control trials found in the dataset.\")\n",
    "lines.append(\"\\n## Figures\\n\")\n",
    "figs = [\n",
    "    \"likert_hist_per_expert.png\",\n",
    "    \"likert_hist_by_type_overall.png\",\n",
    "    \"likert_agreement_spearman.png\",\n",
    "    \"likert_agreement_kendall.png\",\n",
    "    \"likert_auc_per_expert.png\",\n",
    "    \"afc_accuracy_per_expert.png\",\n",
    "    \"afc_fool_rate_per_expert.png\",\n",
    "    \"control_position_bias.png\",\n",
    "    \"likert_auc_vs_foolrate.png\",\n",
    "    \"per_synthetic_image_foolability_top15.png\",\n",
    "]\n",
    "for f in figs: lines.append(f\"- {f}\")\n",
    "lines.append(\"\\n## Do Likert separations predict who gets fooled?\\n\")\n",
    "lines.append(f\"- Spearman correlation between per-expert Likert AUC and fool rate: ρ={fmt(rho)}, p={fmt(rho_p)}.\")\n",
    "lines.append(\"\\n## Which synthetic images are most convincing?\\n\")\n",
    "if len(per_synth_image):\n",
    "    head = per_synth_image.head(10)\n",
    "    for _, r in head.iterrows():\n",
    "        lines.append(f\"- `{r['synthetic_image']}`: foolability {pct(r['foolability'])} over {int(r['n_trials'])} trials; mean Likert {fmt(r['mean_rating'])} (n={int(r['n_ratings']) if pd.notna(r['n_ratings']) else 0}).\")\n",
    "else:\n",
    "    lines.append(\"- No mixed 2AFC trials found to assess synthetic-image foolability.\")\n",
    "\n",
    "report_path = os.path.join(RESULTS_DIR, \"can_we_trick_the_experts_report.md\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Markdown report written to:\", report_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8) Most/least convincing synthetic images",
   "id": "1fb7ccbc6cb14ee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Real/Synthetic pairs: zero-gap + best/worst examples ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "REAL_DIR = \"../data/SynMT/real/small/single_frame\"\n",
    "SYN_DIR  = \"../data/SynMT/synthetic/validation/images\"\n",
    "\n",
    "def resolve_path(filename: str, typ: str):\n",
    "    \"\"\"Find file by exact/basename in the respective tree.\"\"\"\n",
    "    p = Path(filename)\n",
    "    if p.exists():\n",
    "        return p\n",
    "    root = Path(REAL_DIR if typ == \"real\" else SYN_DIR)\n",
    "    hits = list(root.rglob(Path(filename).name))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def read_rgb(path: Path):\n",
    "    \"\"\"Read image as RGB numpy array.\"\"\"\n",
    "    with Image.open(path) as im:\n",
    "        return np.array(im.convert(\"RGB\"))\n",
    "\n",
    "def concat_side_by_side(imgL: np.ndarray, imgR: np.ndarray, sep_px: int = 0):\n",
    "    \"\"\"Concatenate two images horizontally with optional tiny separator.\"\"\"\n",
    "    hL, wL = imgL.shape[:2]\n",
    "    hR, wR = imgR.shape[:2]\n",
    "    target_h = min(hL, hR)\n",
    "    # Resize both to the same height\n",
    "    imL = Image.fromarray(imgL).resize((int(wL * target_h / hL), target_h), Image.BICUBIC)\n",
    "    imR = Image.fromarray(imgR).resize((int(wR * target_h / hR), target_h), Image.BICUBIC)\n",
    "    imL = np.array(imL); imR = np.array(imR)\n",
    "    if sep_px <= 0:\n",
    "        return np.hstack([imL, imR])\n",
    "    sep = np.ones((target_h, sep_px, 3), dtype=np.uint8) * 255\n",
    "    return np.hstack([imL, sep, imR])\n",
    "\n",
    "def show_composites(items, title, outpath, cols=3, max_items=9, sep_px: int = 0):\n",
    "    \"\"\"\n",
    "    items: list of dicts with keys { 'real_path', 'syn_path', 'caption' }\n",
    "    \"\"\"\n",
    "    sel = items[:max_items]\n",
    "    if not sel:\n",
    "        print(f\"[info] no items to display for {title}\")\n",
    "        return\n",
    "    rows = int(np.ceil(len(sel) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5.8, rows * 4.2))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes[len(sel):]:\n",
    "        ax.axis(\"off\")\n",
    "    for ax, item in zip(axes, sel):\n",
    "        rpath, spath = item[\"real_path\"], item[\"syn_path\"]\n",
    "        if rpath is None or spath is None:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        imgR = read_rgb(rpath)\n",
    "        imgS = read_rgb(spath)\n",
    "        comp = concat_side_by_side(imgR, imgS, sep_px=sep_px)  # zero or tiny separator\n",
    "        ax.imshow(comp)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(item[\"caption\"], fontsize=5)\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=220, bbox_inches=\"tight\")\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---- Build per-synthetic statistics from 2AFC mixed trials ----\n",
    "if \"pair_kind\" not in afc.columns:\n",
    "    afc[\"pair_kind\"] = np.where(afc[\"image1_type\"] == afc[\"image2_type\"], afc[\"image1_type\"], \"mixed\")\n",
    "\n",
    "mixed = afc[afc[\"pair_kind\"] == \"mixed\"].copy()\n",
    "\n",
    "def get_syn_real(row):\n",
    "    if row[\"image1_type\"] == \"synthetic\":\n",
    "        return row[\"image1_filename\"], row[\"image2_filename\"]\n",
    "    else:\n",
    "        return row[\"image2_filename\"], row[\"image1_filename\"]\n",
    "\n",
    "syn_files = []\n",
    "real_files = []\n",
    "for _, row in mixed.iterrows():\n",
    "    s, r = get_syn_real(row)\n",
    "    syn_files.append(s); real_files.append(r)\n",
    "mixed[\"synthetic_image\"] = syn_files\n",
    "mixed[\"real_image\"] = real_files\n",
    "mixed[\"synthetic_won\"] = (mixed[\"chosen_image_type\"] == \"synthetic\").astype(int)\n",
    "\n",
    "# Foolability and representative real counterpart\n",
    "per_synth = (mixed.groupby(\"synthetic_image\")[\"synthetic_won\"]\n",
    "                  .agg(foolability=\"mean\", n_trials=\"count\")\n",
    "                  .reset_index())\n",
    "\n",
    "# Most common real counterpart per synthetic (for display)\n",
    "real_for_synth = (mixed.groupby([\"synthetic_image\", \"real_image\"])\n",
    "                       .size().reset_index(name=\"n\")\n",
    "                       .sort_values([\"synthetic_image\",\"n\"], ascending=[True, False])\n",
    "                       .drop_duplicates(\"synthetic_image\")\n",
    "                       .set_index(\"synthetic_image\")[\"real_image\"]\n",
    "                 )\n",
    "\n",
    "per_synth = per_synth.merge(real_for_synth, on=\"synthetic_image\", how=\"left\")\n",
    "\n",
    "# Add Likert realism for synthetic images if available\n",
    "likert_by_image = (likert.groupby([\"image_filename\",\"image_type\"])[\"rating_score\"]\n",
    "                          .agg(mean_rating=\"mean\", n_ratings=\"count\")\n",
    "                          .reset_index())\n",
    "synth_likert = likert_by_image[likert_by_image[\"image_type\"] == \"synthetic\"][[\"image_filename\",\"mean_rating\",\"n_ratings\"]]\n",
    "per_synth = per_synth.merge(synth_likert, left_on=\"synthetic_image\", right_on=\"image_filename\", how=\"left\")\n",
    "per_synth.drop(columns=[\"image_filename\"], inplace=True)\n",
    "\n",
    "# ---- Build best and worst lists ----\n",
    "TOPK = 9\n",
    "best = per_synth.sort_values(\"foolability\", ascending=False).head(TOPK)\n",
    "worst = per_synth.sort_values(\"foolability\", ascending=True).head(TOPK)\n",
    "\n",
    "def to_items(df):\n",
    "    items = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        syn = resolve_path(r.synthetic_image, \"synthetic\")\n",
    "        rea = resolve_path(r.real_image, \"real\")\n",
    "        cap = f\"{Path(r.synthetic_image).name}\\nFoolability={r.foolability:.2f} (n={int(r.n_trials)})\"\n",
    "        if pd.notna(r.mean_rating):\n",
    "            cap += f\"; Likert≈{r.mean_rating:.2f} (n={int(r.n_ratings)})\"\n",
    "        items.append({\"real_path\": rea, \"syn_path\": syn, \"caption\": cap})\n",
    "    return items\n",
    "\n",
    "best_items = to_items(best)\n",
    "worst_items = to_items(worst)\n",
    "\n",
    "# ---- Display & save panels (zero gap) ----\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "show_composites(best_items,  \"BEST synthetic (highest foolability)\",  os.path.join(RESULTS_DIR, \"pairs_best.png\"),  cols=3, max_items=TOPK, sep_px=0)\n",
    "show_composites(worst_items, \"WORST synthetic (lowest foolability)\", os.path.join(RESULTS_DIR, \"pairs_worst.png\"), cols=3, max_items=TOPK, sep_px=0)\n"
   ],
   "id": "3eef28b0dfeb15c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85d5a6cbccad330e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
